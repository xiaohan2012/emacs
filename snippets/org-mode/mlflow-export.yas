# -*- mode: snippet -*-
# name: mlflow-export
# key: mlfexp
# --


#+begin_src python :results value raw :return tabulate(df, headers=df.columns, tablefmt='orgtbl')
  from tabulate import tabulate
  import pandas as pd
  import mlflow
  from mlflow.tracking.client import MlflowClient
  from mlflow.entities import ViewType
  from flatten_dict import flatten

  EXPERIMENT_NAME = ''
  METRIC_FOR_RANKING = ''
  TRACKING_URI = 'file:///path/to/your/mlruns'
  cli = MlflowClient(tracking_uri=TRACKING_URI)
  mlflow.set_tracking_uri(TRACKING_URI)
  experiment_id = mlflow.get_experiment_by_name(EXPERIMENT_NAME).experiment_id
  runs = cli.search_runs(
      experiment_ids=experiment_id,
      filter_string="your filter string",
      run_view_type=ViewType.ACTIVE_ONLY,
  )
  rows = []
  for run in runs:
      one_row = {'M': run.data.metrics, 'P': run.data.params, 'dataset': run.data.tags['dataset']}
      rows.append(flatten(one_row, reducer='path'))

  df = pd.DataFrame.from_records(rows)
  hyperparam_keys = [key for key in rows[0].keys() if key.startswith('P')]
  summ_raw = df.groupby(hyperparam_keys + ['dataset'])['M/{}'.format(METRIC_FOR_RANKING)].describe()[['mean', 'std']]
  summ_raw = summ_raw.reset_index()
  summ = summ_raw[['dataset', 'mean', 'std']].sort_values(['mean'], ascending=False).groupby('dataset').head(1)
  summ = summ.rename({'mean': '{}/mean'.format(METRIC_FOR_RANKING), 'std': '{}/std'.format(METRIC_FOR_RANKING)}, axis=1)
  return (summ.to_markdown(index=False))
#+end_src
